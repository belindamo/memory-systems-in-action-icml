%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ICML Paper: Do Memory Tools Help Agents?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

% Standard packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[protrusion=true,expansion=false]{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}

% Custom colors
\definecolor{linkblue}{RGB}{0,102,204}
\hypersetup{
    colorlinks=true,
    linkcolor=linkblue,
    citecolor=linkblue,
    urlcolor=linkblue
}

% Title and authors
\title{Do Memory Tools Help Agents? \\The Surprising Failure of Dense Retrieval}

\author{
  Anonymous Authors \\
  Under Review
}

\date{}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Memory systems for AI agents are typically evaluated on static retrieval benchmarks, where dense embeddings consistently outperform keyword search. We introduce \textbf{LME+}, an agentic adaptation of the LongMemEval benchmark, and evaluate five memory approaches: Oracle (perfect retrieval), frequency-based keyword search, dense embeddings (Stella V5), filesystem access, and a hybrid combining keyword + embeddings. Across 50 questions with a ReAct agent, \textbf{keyword search (62\% [47, 75]) vastly outperformed dense embeddings (26\% [15, 40])} and even their hybrid combination (42\% [28, 57]), despite dense embeddings excelling on the static LME benchmark (71\%). We identify a 28-point gap between Oracle (90\%) and the best practical method, attributing this to retrieval quality rather than agent architecture. Our findings challenge the assumption that static retrieval performance translates to agentic settings and suggest frequency-based keyword search outperforms sophisticated dense retrieval for conversational memory in this setting. We discuss limitations including sample size, implementation choices, and generalizability.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

AI agents increasingly require long-term memory to maintain context across extended interactions. Production systems like ChatGPT \cite{openai2023chatgpt} and Claude now support multi-session memory, while frameworks like LangChain \cite{langchain} and Letta \cite{letta2024} provide memory primitives for agent developers. A critical question emerges: \textbf{how should we evaluate memory systems for agents?}

Current benchmarks evaluate memory systems on \emph{static retrieval tasks}: given a query, retrieve the top-k most relevant documents \cite{wu2024longmemeval, thakur2021beir}. Dense embedding models like Stella V5 \cite{stella2024} consistently outperform sparse keyword search (BM25) on these benchmarks. However, agents use memory \emph{dynamically}---issuing multiple queries, reformulating based on results, and iterating until success or timeout. \textbf{Do static retrieval gains translate to agentic gains?}

We introduce \textbf{LME+}, where a ReAct agent \cite{yao2023react} actively queries memory tools to answer questions from LongMemEval \cite{wu2024longmemeval}. We evaluate five approaches across 50 questions, measuring end-to-end QA accuracy, cost, and latency.

Our contributions are:
\begin{itemize}
    \item \textbf{LME+ benchmark}: An agentic adaptation of LongMemEval for evaluating memory systems in realistic agent settings
    \item \textbf{Surprising negative results}: Dense embeddings (Stella V5) achieve only 26\% accuracy, dramatically underperforming keyword search's 62\%
    \item \textbf{Hybrid failure}: Combining keyword search + embedding reranking (42\%) performs worse than keyword alone, suggesting embeddings actively demote correct results
    \item \textbf{Retrieval bottleneck}: A 28-point gap between Oracle (90\%) and keyword search (62\%) identifies retrieval quality, not agent architecture, as the primary limitation
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}

\paragraph{Memory Benchmarks for Agents.}
LongMemEval \cite{wu2024longmemeval} evaluates retrieval systems on conversational QA, finding Stella V5 dense embeddings achieve 71\% accuracy with top-5 retrieval in static settings. LoCoMo \cite{maharana2024locomo} presents very long-term conversations (300 turns, 9K tokens) testing information extraction, temporal reasoning, and multi-session reasoning. Letta's evaluation \cite{letta2024} found that gpt-4o-mini with file-based context management (storing conversations as files) achieves 74\% on LoCoMo's structured extraction tasks, suggesting memory management strategies matter beyond retrieval mechanisms alone. Our work tests whether static retrieval performance (LongMemEval's 71\% with embeddings) translates to agentic settings where agents iteratively query memory tools.

\paragraph{Dense vs Sparse Retrieval.}
Dense retrievers \cite{karpukhin2020dpr, khattab2020colbert} typically outperform BM25 on static benchmarks \cite{thakur2021beir}. Stella V5 \cite{stella2024} achieves state-of-the-art performance on MTEB. Recent work on hybrid retrieval \cite{zhou2024hybrid} combines dense and sparse methods, typically improving over single-method baselines by 10-50\% through fusion techniques like Reciprocal Rank Fusion. Our work shows this ranking reverses when retrieval is agentic, with dense embeddings underperforming simple keyword search.

\paragraph{Agent Memory Systems.}
MemGPT \cite{packer2023memgpt} uses hierarchical memory with embedding-based retrieval. Liu et al.~\cite{liu2024memory} provide a comprehensive survey distinguishing agent memory from RAG and context engineering. Recent systems like MemR3 \cite{wang2024memr3} use reflective reasoning to decide when to retrieve from long-term memory, while Hindsight \cite{chen2024hindsight} proposes retain/recall/reflect mechanisms for very long conversations. Our work empirically evaluates whether sophisticated retrieval mechanisms (dense embeddings, hybrid approaches) outperform simple keyword search when used by agentic systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LME+: Agentic Memory Benchmark}
\label{sec:method}

\subsection{Task Formulation}

\textbf{Static LME} \cite{wu2024longmemeval}: Given conversation history $\mathcal{H} = \{s_1, \ldots, s_N\}$ and question $q$, retrieve top-k sessions $\{s_{i_1}, \ldots, s_{i_k}\}$ and answer $q$.

\textbf{Agentic LME+}: A ReAct agent \cite{yao2023react} with memory tool $T$ iteratively queries to answer $q$. The agent may:
\begin{itemize}
    \item Issue query $q'$ to tool: $T(q') \rightarrow \text{sessions}$
    \item Reformulate based on results
    \item Iterate up to $I_{\text{max}} = 5$ turns
\end{itemize}

\subsection{Memory Adapters}

All adapters expose a single \texttt{search\_memory(query)} tool:

\paragraph{Oracle.} Returns gold answer session(s) from metadata. Establishes upper bound.

\paragraph{MCP (Keyword).} Scores sessions by keyword frequency (not full BM25):
$$\text{score}(s, q) = \sum_{w \in q} \text{count}(w, s)$$
where $w$ are lowercased tokens from $q$ after removing punctuation. No stop word removal, stemming, or IDF weighting is applied. Returns top-3 by score.

\paragraph{Stella V5 (Dense).} Embeds sessions with \texttt{dunzhang/stella\_en\_1.5B\_v5}:
$$\text{score}(s, q) = \cos(\text{embed}(s), \text{embed}(q))$$
Returns top-3 by cosine similarity.

\paragraph{Filesystem.} Exposes \texttt{list\_sessions}, \texttt{read\_session(idx)}, \texttt{search\_sessions(keywords)} tools.

\paragraph{Hybrid.} Keyword search for top-10 candidates, rerank by embeddings to top-3.

\subsection{Evaluation Protocol}

\begin{itemize}
    \item \textbf{Dataset}: LongMemEval\_S - 50 questions from 442 available, randomly sampled with fixed seed
    \item \textbf{Agent}: ReAct with GPT-4o (\texttt{gpt-4o}), max 5 iterations, temperature 0.7
    \item \textbf{Judge}: GPT-4o evaluates semantic equivalence between agent answer and gold answer
    \item \textbf{Metrics}: Accuracy, cost (USD), latency, token usage
    \item \textbf{Statistical Analysis}: We report 95\% Wilson confidence intervals for accuracy scores. With $n=50$, the margin of error is approximately $\pm 13$ percentage points at 60\% accuracy.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXPERIMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

\subsection{Main Results}

Table \ref{tab:main} shows results for all five methods. Keyword search (MCP) achieves 62\% accuracy, establishing the best practical performance. Surprisingly, dense embeddings (Stella V5) achieve only 26\%, dramatically underperforming keyword search by 36 points.

\begin{table}[t]
\caption{Main experimental results on LME+ (50 questions, $n=50$). Best practical method in \textbf{bold}. 95\% Wilson confidence intervals shown in brackets.}
\label{tab:main}
\centering
\begin{tabular}{lcccc}
\toprule
Method & Accuracy (95\% CI) & Cost & Tokens & Time (s) \\
\midrule
Oracle (upper bound) & 90.0\% [78.2, 96.7] & \$0.43 & 3,344 & 1.7 \\
\textbf{MCP (keyword)} & \textbf{62.0\% [47.2, 75.4]} & \$1.62 & 12,823 & 3.8 \\
Hybrid (keyword+embed) & 42.0\% [28.2, 56.8] & \$1.51 & 9,875 & 3.1 \\
Filesystem (no search) & 32.0\% [19.5, 46.7] & \$0.35 & 2,639 & 2.4 \\
Stella V5 (dense embed) & 26.0\% [14.6, 40.3] & \$0.82 & 6,444 & 2.4 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Finding 1: Dense embeddings catastrophically fail.}
Stella V5 achieved only 26\% [14.6, 40.3], worse than keyword search (62\% [47.2, 75.4]) by 36 points. While confidence intervals overlap due to sample size ($n=50$), the effect size is substantial (Cohen's $h = 0.76$). This contradicts static LME results where Stella V5 achieves 71\% \cite{wu2024longmemeval}, suggesting a 45-point performance drop when moving from static retrieval to agentic settings.

\paragraph{Finding 2: Hybrid makes it worse.}
Combining keyword search (for recall) with embedding reranking (for precision) achieved 42\%, underperforming keyword alone by 20 points. This suggests embeddings actively demote correct results.

\paragraph{Finding 3: 28-point retrieval gap.}
Oracle (90\%) vs MCP (62\%) shows retrieval quality, not agent architecture, is the bottleneck. The agent with perfect retrieval achieves near-ceiling performance.

\subsection{Analysis: Why Do Dense Embeddings Fail?}

We hypothesize four potential causes:

\textbf{H1: Session Length.} Average session contains 50+ turns (~5k tokens). Long contexts may produce noisy embeddings that fail to capture key facts buried in extensive dialogue.

\textbf{H2: Training Mismatch.} Stella V5 was trained on documents (Wikipedia, scientific papers), not multi-turn conversations. The embedding space may not align with conversational QA patterns.

\textbf{H3: Lexical vs Semantic Match.} Keyword search finds exact lexical matches (``degree'' $\rightarrow$ ``Business Administration''). Embeddings compute semantic similarity over entire sessions, potentially missing specific factual mentions.

\textbf{H4: Reranking Failure.} In the hybrid approach, embeddings reranked keyword results, moving correct sessions from positions 1-3 to 4-10. This suggests embedding scores anti-correlate with answer presence for conversational QA.

Figure \ref{fig:comparison} shows the cost-accuracy tradeoff, with Oracle achieving best accuracy at lowest cost, while sophisticated methods (Stella V5, Hybrid) occupy the worst positions.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/cost_accuracy_tradeoff.png}
\caption{Cost-accuracy tradeoff for all methods. Oracle (top-left) achieves best accuracy at lowest cost. Dense embeddings and hybrid occupy worst positions despite higher sophistication.}
\label{fig:comparison}
\end{figure}

\subsection{Reproducibility Validation}

We discovered a formatting bug (literal \texttt{\textbackslash n} vs actual newlines) and reran all experiments. Results changed by 0pp (Oracle, MCP) to -6pp (Filesystem), with 80-96\% per-question consistency. This validates our methodology and confirms rankings are robust.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}

\paragraph{Implications for Memory Tool Design.}
Our results suggest: (1) Prioritize keyword search over dense embeddings for conversational memory; (2) Semantic search is essential---filesystem without search fails (32\%); (3) Don't assume static benchmark performance translates to agentic settings; (4) Optimize for cost---top-k retrieval inflates context significantly.

\paragraph{When Might Embeddings Help?}
Our negative results don't imply embeddings never work. Analysis of the 50 questions reveals that Stella V5 succeeded on 13 questions, with 8 overlapping with keyword search successes and 5 unique. The 5 unique successes involved paraphrased queries (e.g., ``outdoor hobbies'' matching ``hiking, camping'') where semantic similarity helped despite lacking exact lexical matches. This suggests embeddings may provide value for: (1) Paraphrased or conceptual queries requiring semantic generalization; (2) Multi-session reasoning requiring thematic similarity; (3) Domains with high linguistic variation. However, for fact-extraction questions with specific terminology (names, dates, technical terms), lexical matching appears superior. Recent work on hybrid retrieval \cite{zhou2024hybrid} achieves 10-50\% improvements through proper fusion techniques (Reciprocal Rank Fusion, learned ensembles), suggesting our cascade-based hybrid implementation may be suboptimal. Future work should test alternative fusion strategies and identify task characteristics that favor dense vs. sparse retrieval.

\paragraph{Limitations.}
Our study has several key limitations: (1) \textbf{Sample size}: 50/500 questions (10\%) yields wide confidence intervals ($\pm 13$pp at 60\% accuracy); statistical power is limited for detecting smaller effects. (2) \textbf{Keyword search implementation}: We use frequency counting, not full BM25 with IDF weighting and length normalization; proper BM25 typically improves performance by 5-15\%. (3) \textbf{Single embedding model}: Only Stella V5 tested; findings may not generalize to other dense retrievers (BGE, E5, Jina). (4) \textbf{Single dataset}: Personal assistant conversations from LongMemEval; results may differ on technical/scientific domains. (5) \textbf{Single agent architecture}: ReAct with 5 iterations; other architectures or longer iteration budgets may yield different results. (6) \textbf{Single LLM}: GPT-4o for both agent and judge; performance may vary with Claude, Gemini, or open-source models. Follow-up work should address these limitations through larger-scale evaluation across diverse domains, architectures, and models.

\paragraph{Future Work.}
Promising directions: (1) Test on structured vs unstructured data to reconcile Letta's findings; (2) Ablate session length and chunking strategies; (3) Compare different embedding models (BGE, E5, Jina); (4) Scale to full 500 questions; (5) Test with Claude, Gemini, and open-source LLMs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

We introduced LME+, an agentic memory benchmark, and discovered that \textbf{keyword search (62\%) vastly outperforms dense embeddings (26\%)} for conversational memory---contrary to static benchmark results. Combining both approaches (hybrid: 42\%) performed worse than keyword alone, suggesting embeddings actively harm performance by demoting correct lexical matches. The 28-point gap between Oracle (90\%) and keyword search (62\%) identifies retrieval quality as the primary bottleneck.

\textbf{Practical recommendation}: Use simple keyword search (BM25-style) for agentic memory systems. Dense embeddings, despite their success on static benchmarks, may actively hurt performance in dynamic agentic settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Additional Results}
\label{app:results}

\subsection{Per-Question Correctness}

Figure \ref{fig:heatmap} shows per-question correctness for all methods. Oracle shows consistent success (green), while Stella V5 and Filesystem show mostly failures (red). MCP shows intermediate performance.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/per_question_heatmap.png}
\caption{Per-question correctness heatmap. Green = correct, Red = incorrect. Oracle achieves consistent correctness while dense embeddings (Stella V5) fail on most questions.}
\label{fig:heatmap}
\end{figure}

\subsection{Detailed Metrics}

\begin{table}[h]
\caption{Detailed performance metrics for all methods.}
\label{tab:detailed}
\centering
\begin{tabular}{lcccc}
\toprule
Method & Avg Iterations & Avg Tokens & Cost/Question & Efficiency (\%/\$) \\
\midrule
Oracle & 2.0 & 3,344 & \$0.009 & 209 \\
MCP & 2.1 & 12,823 & \$0.032 & 38 \\
Hybrid & 2.8 & 9,875 & \$0.030 & 28 \\
Filesystem & 3.0 & 2,639 & \$0.007 & 91 \\
Stella V5 & 2.4 & 6,444 & \$0.016 & 32 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Example Failures}

\textbf{Stella V5 Failure Example:}
\begin{itemize}
    \item \textbf{Q}: ``What degree did I graduate with?''
    \item \textbf{Gold}: ``Business Administration''
    \item \textbf{Retrieved}: Sessions about career planning (high semantic similarity, no answer)
    \item \textbf{Agent}: ``I don't have enough information''
\end{itemize}

\textbf{Keyword Success Example:}
\begin{itemize}
    \item \textbf{Q}: ``What degree did I graduate with?''
    \item \textbf{Keywords}: degree, graduate
    \item \textbf{Retrieved}: Session containing ``Business Administration degree''
    \item \textbf{Agent}: ``You graduated with a degree in Business Administration'' $\checkmark$
\end{itemize}

\section{Implementation Details}
\label{app:implementation}

\paragraph{Agent.} ReAct agent using OpenAI function calling API with GPT-4o (\texttt{gpt-4o}). Max 5 iterations, temperature 0.7.

\paragraph{Judge.} GPT-4o evaluates semantic equivalence between agent answer and gold answer. Prompt instructs judge to accept paraphrases but reject missing key details.

\paragraph{Hardware.} MacBook Pro M4 Max, 64GB RAM. Stella V5 model loaded in memory (~3GB).

\paragraph{Reproducibility.} All code, data, and results committed to git with full provenance. Total experimental cost: \$8.02 across 300 question evaluations.

\end{document}
