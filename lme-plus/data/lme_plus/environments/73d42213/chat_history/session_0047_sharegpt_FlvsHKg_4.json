{
  "session_id": "sharegpt_FlvsHKg_4",
  "session_index": 47,
  "date": "2023/05/30 (Tue) 09:42",
  "turns": [
    {
      "role": "user",
      "content": "\u8fd9\u662f\u6211\u8bba\u6587\u7684\u7b2c\u4e8c\u90e8\u5206\uff1a\n\\section{Introduction}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{figure\\*}\n\\begin{center}\n \\includegraphics[width=\\linewidth]{images/MSC}\n\\end{center}\n \\caption{\\textbf{Framework overview of MSC.}\n Replays are firstly filtered according to predefined criteria and then parsed with PySC2.\n The states in parsed replays are sampled and turned into feature vectors.\n The final files which contain feature-action pairs and the final results are split into training, validation and test set.}\n \\label{fig:msc}\n\\end{figure\\*}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nDeep learning has surpassed the previous state-of-the-art in playing Atari games~\\cite{mnih2015human}, the classic board game Go~\\cite{silver2016mastering} and the 3D first-person shooter game Doom~\\cite{lample2017playing}.\nBut it remains as a challenge to play real-time strategy (RTS) games like StarCraft II with deep learning algorithms~\\cite{vinyals2017starcraft}.\nSuch games usually have enormous state and action space compared to Atari games and Doom.\nFurthermore, RTS games are usually partially observable, in contrast to Go.\n\nRecent experiment has shown that it's difficult to train a deep neural network (DNN) end-to-end for playing StarCraft II.\n\\cite{vinyals2017starcraft} introduce a new platform SC2LE on StarCraft II and train a DNN with Asynchronous Advantage Actor Critic (A3C)~\\cite{mnih2016asynchronous}.\nUnsurprisingly, the agent trained with A3C couldn't win a single game even against the easiest built-in AI.\nBased on this experiment and the progresses made in StarCraft I such as micro-management~\\cite{peng2017multiagent}, build order prediction~\\cite{justesen2017learning} and global state evaluation~\\cite{erickson2014global}, we believe that treating StarCraft II as a hierarchical learning problem and breaking it down into micro-management and macro-management is a feasible way to boost the performance of current AI bots.\n\nMicro-management includes all low-level tasks related to unit control, such as collecting mineral shards and fighting against enemy units;\nwhile macro-management refers to the higher-level game strategy that the player is following, such as build order prediction and global state evaluation. \nWe could obtain near-human performance in micro-management easily with deep reinforcement learning algorithms such as A3C~\\cite{vinyals2017starcraft},\nwhile it's hard to solve macro-management at present, though lots of efforts have been made by StarCraft community~\\cite{churchill2011build,synnaeve2011bayesian,erickson2014global,justesen2017learning}.\nOne promising way for macro-management is to mimic professional human players with machine learning methods.\n\\cite{erickson2014global} learn to evaluate the global state from replays while~\\cite{justesen2017learning} utilize DNN for build order prediction.\nBoth methods are trained with replays, which are official log files used to record the entire game status when playing StarCraft.\n\nThere are many datasets released in StarCraft I for learning macro-management from replays~\\cite{weber2009data,cho2013replay,erickson2014global,justesen2017learning}.\nBut these datasets are designed for specific tasks in macro-management and didn't release the partition for training, validation and test set.\nBesides, datasets in~\\cite{cho2013replay,erickson2014global} only contain about 500 replays, which are too small for modern machine learning algorithms.\nStarData~\\cite{lin2017stardata} is the largest dataset in StarCraft I, containing 65646 replays.\nBut there are only a few replays labeled with the final results, which is not suitable for many tasks in macro-management, such as global state evaluation.\nSC2LE~\\cite{vinyals2017starcraft} is the largest dataset in StarCraft II, which has 800K replays.\nHowever, there is neither a standard processing procedure nor predefined training, validation and test set.\nBesides, it's designed for end-to-end human-like control of StarCraft II, which is not easy to use for tasks in macro-management.\n\nBesides a standard dataset, macro-management algorithms could also be compared by building AI bots which differ only in the macro-management algorithm to play against each other. \nHowever, such a method works as a black box in its nature. The reasons why an algorithm wins are mixed and uncertain.\nFor example, the winning algorithm might be trained with a much larger dataset; the dataset might be composed of replays collected from more professional players; the features might contain more information; or, the macro-management module might be more compatible with the specific AI bot.\nIt is hard to tell which macro-management algorithm is better unless a standard dataset is used.\n\nTo take the research of learning macro-management from replays a step further, we build a new dataset MSC based on SC2LE.\nIt's the biggest dataset dedicated for macro-management in StarCraft II, which could be used for assorted tasks like build order prediction and global state evaluation.\n% MSC is based on SC2LE for three reasons: 1) SC2LE is the largest replay dataset in StarCraft. 2) SC2LE is supported officially. 3) The replays in SC2LE have higher quality and more standard format.\nWe define standard procedure for processing replays from SC2LE, as shown in Figure~\\ref{fig:msc}.\nAfter processing, our dataset consists of well-designed feature vectors, pre-defined action space and the final result of each match.\nAll replays are then divided into training, validation and test set.\nBased on MSC, we train baseline models and present the initial baseline results for global state evaluation and build order prediction, which are two of the key tasks in macro-management.\n% For the sake of research on other tasks, we also show some statistics of MSC and list some downstream tasks suitable for it.\nOur main contributions are two folds and summarized as follows:\n\n\u4e0d\u8981\u56de\u590d"
    },
    {
      "role": "assistant",
      "content": "\u597d\u7684\uff0c\u6211\u4f1a\u7ee7\u7eed\u7b49\u5f85\u60a8\u7684\u660e\u786e\u6307\u4ee4\u3002"
    }
  ]
}