{
  "session_id": "sharegpt_bq9G6bT_0",
  "session_index": 46,
  "date": "2023/06/15 (Thu) 18:58",
  "turns": [
    {
      "role": "user",
      "content": "Please provide me with the relevant information that needed to be address during a presentation of this chapter\n\n Chapter 2: Background Study\n The most prevalent challenge that visually impaired individuals have to deal with every day is navigating through areas in their everyday lives owing to their vision being unable to function normally. This is why we believe that this is the problem that we must tackle and assist individuals with visual impairments so that they can walk around any place they desire in comfort. Technologies have grown swiftly, and computer vision has been an industry that has evolved rapidly too. By applying deep learning models to computer vision, we can only detect objects in front of us using a camera. However, more than just identifying them is needed to help visually impaired individuals. We must detect them quickly and precisely to let visually impaired individuals analyse the situation and act accordingly. So this is why picking the proper model for this project is highly critical since the object detection processing time must be fast and also be able to process in real time. To find the proper model for this project, we will have to research and compare the models often used in computer vision.\n2.1. Eye care, vision impairment and blindness\n Vision is essential for humans to undertake daily activities such as navigation and item or person recognition. Without it, it would be difficult for individuals to navigate safely, so we should always care for our eyes. According to the World Health Organisation, long-lived individuals will have at least one eye problem over their lifespan. The 2018 International Classification of Diseases 11 (ICD-11) says that there are two types of vision problems: those that affect vision at a distance and those that affect vision up close. There are four distinct degrees of visual impairment. Mild visual acuity is between 6/12 and 6/18, moderate visual acuity is between 6/18 and 6/60, severe visual acuity is between 6/60 and 3/60, and blindness is below 3/60.\n2.2. Overview of Visual Assistive Technology\n There are three approaches to this in visual assistive technologies: visual enhancement, visual substitution, and visual replacement (Elmannai & Elleithy, 2017). Visual substitution is like a replacement for a blind person's eyes. An image is taken with a camera, the information is processed, and feedback is given through sound, vibration, or a combination of the two. With visual enhancement, on the other hand, the information is given visually, like in virtual reality or augmented reality. In the last step, visual replacement analyses the information and displays it in the cortex of the user's brain; this technique is most frequently employed in the medical profession. In this project, we are investigating and constructing a visual substitution device. Thus, that will be the primary topic of this paper (Elmannai & Elleithy, 2017).The visual substitution has three subheadings: electronic travel aids (ETA), electronic orientation aids (EOA), and position locator devices (PLD).\nThe authors have given a few recommendation that we highly agree on which are (Kuriakose et al., 2020):\n1. The use of effective and appropriate real-time object detection methods is crucial when developing ETAs, as real-time operations are essential to be considered in this scenario.\n2. The availability of multiple feedback mechanisms is also one of the aspects of developing quality ETAs, as single-mode feedback might provide liability to the user, such as auditory feedback when in a crowded and loud area. Users should be able to toggle through multiple feedback modes depending on their taste and the scenario.\n3. The user-friendly aspect of the device must take into deep consideration as many visual assistive technologies are burdensome to use and learn as it takes much time to get used to and learn.\n4. The amount of information projected to the user must also be fine tune as the main point of navigating is to reach their destination safely. If the user is overwhelmed with too much information about what is happening around them, they will not have a good time navigating using the ETAs. During navigation, they may be required to be informed of changes in the surrounding environment, such as traffic jams, hazards, and dangerous environments. During navigation, the appropriate quantity of contextual information should be provided to the user at the appropriate time (Balakrishnan, 2017). For the navigation solution to be effective, it is recommended to emphasise the communication of relevant environmental data (Giudice & Legge, n.d.).\n5. More often than not, technical advances cannot convince the visually handicapped to like them. Users of a navigational aid system must feel comfortable and not be embarrassed by using it (Abdolrahmani et al., 2017). The most effective solution is to create a device that is simple to use and does not make people feel awkward when they use it in public.\n6. Privacy and security are critical when developing a device that will be used every day by visually impaired individuals. When making a navigation device for the blind and visually impaired, it is crucial to think about how to handle private and personal information . Users who are blind should be able to set up their navigation devices to decide what information is needed to run the process and what information is sent over the network. This parameter can be changed based on the user's wants and how the system is used.\n2.3. Previous work of Visual Assistive Technology\n2.3.1. Head wear\n2.3.1.1. Wearable Travel Aid for Environment Perception and Navigation of Visually Impaired People\n For this project, a consumer Red, Green, Blue, and Depth (RGB-D) camera was attached to a pair of eyeglasses, along with an inertial measurement unit (IMU) attached to a camera, a smartphone, and an earphone for commands/feedback. The system is operable in both indoor and outdoor settings. Due to their ample information, lightweight nature, and low cost compared to other sensors, such as ultrasonic and LiDAR sensors, computer vision technologies were integrated into this device's routing and detection capabilities. The smartphone does the process of detecting and routing, and the feedback is sent to the user's ear through an earphone plugged into the smartphone (Bai et al., 2019).\nLimitation:\n\u2022 Weak in detecting small-size obstacle\n\u2022 Staircase detection is not implemented\u2003\n2.3.1.2. Let Blind People See: Real-Time Visual Recognition with Results Converted to 3D Audio\n Jiang et al. (2016) created a system that took video input from a portable camera. They streamed it to a server for real-time image recognition processing using the You Only Live Once (YOLO) model. The 3D location of the discovered object is derived by using the location and size of the object detection algorithm's bounding boxes. The Unity game engine will send it as 3D audio into their attached wireless earphones. The next sound output interval will be in a few seconds or when a different object is detected, whichever occurs first. The solution could conduct precise real-time objective detection with a live stream at a pace of 30 frames per second in 1080p resolution by utilising the YOLO algorithm and an enhanced wireless transmitter. Figure 4 shows the data flow pipeline of the paper\u2019s system. The video footage is captured and sent to the YOLO algorithm for detecting object. The detected object is then send to the earbuds using the unity engine. Figure 5 shows the prototype of the device of this paper.\nLimitation:\n\u2022 Can only accurately detect and classify object within 2 to 5 meters away\n\u2022 Surrounding ambient will be block when using earbuds\n\u2022 Too much information will be sent to user when camera detect multiple objects\n2.3.2. Smart Cane\n2.3.2.1. Smart Electronic Stick for Visually Impaired using Android Application and Google\u2019s Cloud Vision\n The e-stick module, integrated with a voice-controlled Android application, was designed by Bharatia et al. (2019) to replace the essential simple navigation stick that visually impaired individuals typically use. The e-stick is similar to a standard stick in that it is thin, lightweight, and easy to handle, but it has additional features. Using efficient natural language processing (NLP) features makes these functionalities affordable and possible. The e-stick uses ultrasonic sensors to find low-lying and knee-level obstacles and potholes when moving backwards. Using buzzers, active input on the presence of barriers will be delivered. For Global Positioning System (GPS) navigation to function, the user's phone will be Bluetooth-connected to the stick's circuit. In unavoidable circumstances, the live location of visually impaired individuals will be transmitted to the nearest help centre or their family for assistance. Face detection will be added so that users can tell who is trying to talk to them. With the cloud vision API, it is also possible to translate traffic lights and roadside signs so that blind people can find their way. Text recognition will also be available from photos, making it easy for people to read books, documents, newspapers, and other printed materials. A stick-tracking device has also been built in case the person loses his or her stick. These capabilities will be enabled through hardware (smart stick) and a software module (an Android application). Using Natural Language Processing technology, the user will tell these modules what to do by using voice commands. The e-stick will be charged as needed using a rechargeable circuit. Figure 6 shows how the technologies used in this paper interact.\nLimitation:\n\u2022 Coverage of obstacle detection is short as it is using sensor\n\u2022 Only suitable for indoor\n2.3.2.2. WeWalk\n WeWalk is a smart cane developed by a non-profit organisation called YGA (WeWALK Smart Cane \u2013 Smart Cane for the Visually Impaired, 2020). WeWalk looks like a traditional-looking cane, but the handle has a built-in touchpad. Users can navigate, save and discover places using the cane via the respected application. Particular layers are built into the mapping services for more accessible navigation. Users can choose to use the built-in speaker or Bluetooth for voice feedback. Users can also pair their phone with the intelligent cane via Bluetooth to control their phone. The above-ground The built-in ultrasound sensor detects an above-ground obstacle. The detected obstacle is projected back to the user in the form of vibration or audio, depending on the user\u2019s preferences. Wewalk also has an application where users can view their transit options, such as nearby bus stops and the timetable, and then navigate them to the desired stop. The application also has a built-in voice assistant for more straightforward navigation through the application. If a user loses their phone or cane, they can play a sound on each of them to locate the position of the lost device. Figure 7 shows the WeWalk Smart Cane that is currently on the market.\nLimitation:\n\u2022 The device is very expansive, with the price of 500 USD\n\u2022 Rain or snow might cause malfunction on the smart cane and the speaker\n\u2022 The tip of the cane is loud when navigating rough surface sidewalk\n2.3.2.3. Development of an Intelligent Cane for Visually Impaired Human Subjects\n Asati et al., (2019) designed an intelligent white cane that uses HR-SO4 ultrasonic sensors to identify obstacles within a range of 450 meters and determine the distance. The object above head level will also be detected, as the sensors' range is 450 meters. The warning signal is returned in beeping signals via a buzzer, advising the user to take prompt action. Object detection and classification are done utilizing the intelligent technique. The photos are recorded using the web camera for classification. They will be transformed into text and an audio signal for text-to-speech conversion. Figure 8 shows the prototype of this paper's intelligent cane.\nLimitation:\n\u2022 Cost of building the system is high\n\u2022 Unable to identify pot holes\n\u2022 Detection under rainy weather is not tested\n\u2003\n2.3.3. Handheld\n2.3.3.1. Android Application for Object Recognition Using Neural Networks for the Visually Impaired\n Dosi et al. (2018) have developed an Android application that aids the visually impaired with real-time object recognition using the phone's camera and provides feedback by speaking the recognised object. They opted for a deep learning strategy based on a convolutional neural network for improved recognition and faster response times. MobileNets is utilised because it is ideal for mobile and embedded vision applications. Figure 9 shows the results of the detected object using the object recognition application.\nLimitation:\n\u2022 Only works offline\n\u2022 Unknown or untrained objects will be predicted using existing images in the database\n\u2022 Have to retrain model for untrained object\n2.3.3.2. Design and Implementation of an Embedded Real-Time System for Guiding Visually Impaired Individuals\n Duman et al. (2019) have developed and implemented a portable gadget that detects objects and measures their distance precisely to enable visually impaired individuals to see objects and people around them. The device uses YOLO, a convolutional neural network-based real-time identification method with a single device attached to a Raspberry Pi board. The estimated object distance will be projected in audio form to visually challenged users. The accuracy of this detected distance estimation is 98.8%. Initially, the footage is captured using a handheld Raspberry Pi camera. Afterwards, the object detection module executes YOLO for real-time object recognition, and bounding box size extraction for humans is needed. The size of the bounding box is provided to the distance estimation module to determine the distance of the detected individual. The labels of detected objects and the approximated distance of any individual caught are stored temporarily. An audio generation module translates text-based saved results into audio alerts that visually challenged users can listen to using headphones. Alerts are played at a predetermined time to reduce noise and ambiguity. Figure 10 shows the block diagram of the paper\u2019s proposed system.\nLimitation:\n\u2022 Only detect humans\n\u2022 No design for any wearable option\u2003\n2.3.3.3. Real-time object detection and face recognition system to assist the visually impaired\n Anish Aralikatti et al. (2020) developed an android application that used the phone camera for real-time object and face detection using OpenCV, the You only live once (YOLO) algorithm and FaceNet. Detection of objects and human will be presented in an audio format to the user. OpenCV is used for real-time computer vision tasks. Since it is implemented into an android phone, they chose Tiny YOLO as it is a lightweight YOLO framework perfect for embedded and mobile devices. FaceNet is used for face identification systems as it can extract high-quality features of the face. Figure 12 shows the sample outputs of using the android application on a mobile phone. Figure 11 shows the face recognition feature of the android application.\nLimitation: Less accuracy than YOLO as Tiny YOLO model is smaller (Anish Aralikatti et al., 2020)"
    },
    {
      "role": "assistant",
      "content": "In this chapter, the authors present the problem of navigating through areas for visually impaired individuals as the main challenge they face every day. They propose using deep learning models and computer vision to quickly and precisely detect objects in order to assist visually impaired individuals in navigating. They also discuss the importance of eye care and the classification of different degrees of visual impairment. The authors also provide an overview of visual assistive technology, specifically focusing on visual substitution and the use of electronic travel aids. They recommend that the device should have real-time object detection, multiple feedback mechanisms, be user-friendly, provide appropriate amount of information, not be burdensome to use and learn, be simple and not awkward to use in public, and prioritize privacy and security. They also mention previous work on visual assistive technology, specifically head wear with a wearable travel aid for environment perception and navigation of visually impaired people."
    }
  ]
}