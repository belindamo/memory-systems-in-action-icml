{
  "turn_global_index": 470,
  "session_index": 44,
  "session_id": "sharegpt_bq9G6bT_2",
  "turn_in_session": 0,
  "date": "2023/05/30 (Tue) 23:23",
  "role": "user",
  "content": "Please provide me with the relevant information that needed to be address during a presentation of this chapter\n\n2.3. Previous work of Visual Assistive Technology\n2.3.1. Head wear\n2.3.1.1. Wearable Travel Aid for Environment Perception and Navigation of Visually Impaired People\n For this project, a consumer Red, Green, Blue, and Depth (RGB-D) camera was attached to a pair of eyeglasses, along with an inertial measurement unit (IMU) attached to a camera, a smartphone, and an earphone for commands/feedback. The system is operable in both indoor and outdoor settings. Due to their ample information, lightweight nature, and low cost compared to other sensors, such as ultrasonic and LiDAR sensors, computer vision technologies were integrated into this device's routing and detection capabilities. The smartphone does the process of detecting and routing, and the feedback is sent to the user's ear through an earphone plugged into the smartphone (Bai et al., 2019).\nLimitation:\n\u2022 Weak in detecting small-size obstacle\n\u2022 Staircase detection is not implemented\u2003\n2.3.1.2. Let Blind People See: Real-Time Visual Recognition with Results Converted to 3D Audio\n Jiang et al. (2016) created a system that took video input from a portable camera. They streamed it to a server for real-time image recognition processing using the You Only Live Once (YOLO) model. The 3D location of the discovered object is derived by using the location and size of the object detection algorithm's bounding boxes. The Unity game engine will send it as 3D audio into their attached wireless earphones. The next sound output interval will be in a few seconds or when a different object is detected, whichever occurs first. The solution could conduct precise real-time objective detection with a live stream at a pace of 30 frames per second in 1080p resolution by utilising the YOLO algorithm and an enhanced wireless transmitter. Figure 4 shows the data flow pipeline of the paper\u2019s system. The video footage is captured and sent to the YOLO algorithm for detecting object. The detected object is then send to the earbuds using the unity engine. Figure 5 shows the prototype of the device of this paper.\nLimitation:\n\u2022 Can only accurately detect and classify object within 2 to 5 meters away\n\u2022 Surrounding ambient will be block when using earbuds\n\u2022 Too much information will be sent to user when camera detect multiple objects\n2.3.2. Smart Cane\n2.3.2.1. Smart Electronic Stick for Visually Impaired using Android Application and Google\u2019s Cloud Vision\n The e-stick module, integrated with a voice-controlled Android application, was designed by Bharatia et al. (2019) to replace the essential simple navigation stick that visually impaired individuals typically use. The e-stick is similar to a standard stick in that it is thin, lightweight, and easy to handle, but it has additional features. Using efficient natural language processing (NLP) features makes these functionalities affordable and possible. The e-stick uses ultrasonic sensors to find low-lying and knee-level obstacles and potholes when moving backwards. Using buzzers, active input on the presence of barriers will be delivered. For Global Positioning System (GPS) navigation to function, the user's phone will be Bluetooth-connected to the stick's circuit. In unavoidable circumstances, the live location of visually impaired individuals will be transmitted to the nearest help centre or their family for assistance. Face detection will be added so that users can tell who is trying to talk to them. With the cloud vision API, it is also possible to translate traffic lights and roadside signs so that blind people can find their way. Text recognition will also be available from photos, making it easy for people to read books, documents, newspapers, and other printed materials. A stick-tracking device has also been built in case the person loses his or her stick. These capabilities will be enabled through hardware (smart stick) and a software module (an Android application). Using Natural Language Processing technology, the user will tell these modules what to do by using voice commands. The e-stick will be charged as needed using a rechargeable circuit. Figure 6 shows how the technologies used in this paper interact.\nLimitation:\n\u2022 Coverage of obstacle detection is short as it is using sensor\n\u2022 Only suitable for indoor\n2.3.2.2. WeWalk\n WeWalk is a smart cane developed by a non-profit organisation called YGA (WeWALK Smart Cane \u2013 Smart Cane for the Visually Impaired, 2020). WeWalk looks like a traditional-looking cane, but the handle has a built-in touchpad. Users can navigate, save and discover places using the cane via the respected application. Particular layers are built into the mapping services for more accessible navigation. Users can choose to use the built-in speaker or Bluetooth for voice feedback. Users can also pair their phone with the intelligent cane via Bluetooth to control their phone. The above-ground The built-in ultrasound sensor detects an above-ground obstacle. The detected obstacle is projected back to the user in the form of vibration or audio, depending on the user\u2019s preferences. Wewalk also has an application where users can view their transit options, such as nearby bus stops and the timetable, and then navigate them to the desired stop. The application also has a built-in voice assistant for more straightforward navigation through the application. If a user loses their phone or cane, they can play a sound on each of them to locate the position of the lost device. Figure 7 shows the WeWalk Smart Cane that is currently on the market.\nLimitation:\n\u2022 The device is very expansive, with the price of 500 USD\n\u2022 Rain or snow might cause malfunction on the smart cane and the speaker\n\u2022 The tip of the cane is loud when navigating rough surface sidewalk\n2.3.2.3. Development of an Intelligent Cane for Visually Impaired Human Subjects\n Asati et al., (2019) designed an intelligent white cane that uses HR-SO4 ultrasonic sensors to identify obstacles within a range of 450 meters and determine the distance. The object above head level will also be detected, as the sensors' range is 450 meters. The warning signal is returned in beeping signals via a buzzer, advising the user to take prompt action. Object detection and classification are done utilizing the intelligent technique. The photos are recorded using the web camera for classification. They will be transformed into text and an audio signal for text-to-speech conversion. Figure 8 shows the prototype of this paper's intelligent cane.\nLimitation:\n\u2022 Cost of building the system is high\n\u2022 Unable to identify pot holes\n\u2022 Detection under rainy weather is not tested\n\u2003\n2.3.3. Handheld\n2.3.3.1. Android Application for Object Recognition Using Neural Networks for the Visually Impaired\n Dosi et al. (2018) have developed an Android application that aids the visually impaired with real-time object recognition using the phone's camera and provides feedback by speaking the recognised object. They opted for a deep learning strategy based on a convolutional neural network for improved recognition and faster response times. MobileNets is utilised because it is ideal for mobile and embedded vision applications. Figure 9 shows the results of the detected object using the object recognition application.\nLimitation:\n\u2022 Only works offline\n\u2022 Unknown or untrained objects will be predicted using existing images in the database\n\u2022 Have to retrain model for untrained object\n2.3.3.2. Design and Implementation of an Embedded Real-Time System for Guiding Visually Impaired Individuals\n Duman et al. (2019) have developed and implemented a portable gadget that detects objects and measures their distance precisely to enable visually impaired individuals to see objects and people around them. The device uses YOLO, a convolutional neural network-based real-time identification method with a single device attached to a Raspberry Pi board. The estimated object distance will be projected in audio form to visually challenged users. The accuracy of this detected distance estimation is 98.8%. Initially, the footage is captured using a handheld Raspberry Pi camera. Afterwards, the object detection module executes YOLO for real-time object recognition, and bounding box size extraction for humans is needed. The size of the bounding box is provided to the distance estimation module to determine the distance of the detected individual. The labels of detected objects and the approximated distance of any individual caught are stored temporarily. An audio generation module translates text-based saved results into audio alerts that visually challenged users can listen to using headphones. Alerts are played at a predetermined time to reduce noise and ambiguity. Figure 10 shows the block diagram of the paper\u2019s proposed system.\nLimitation:\n\u2022 Only detect humans\n\u2022 No design for any wearable option\u2003\n2.3.3.3. Real-time object detection and face recognition system to assist the visually impaired\n Anish Aralikatti et al. (2020) developed an android application that used the phone camera for real-time object and face detection using OpenCV, the You only live once (YOLO) algorithm and FaceNet. Detection of objects and human will be presented in an audio format to the user. OpenCV is used for real-time computer vision tasks. Since it is implemented into an android phone, they chose Tiny YOLO as it is a lightweight YOLO framework perfect for embedded and mobile devices. FaceNet is used for face identification systems as it can extract high-quality features of the face. Figure 12 shows the sample outputs of using the android application on a mobile phone. Figure 11 shows the face recognition feature of the android application.\nLimitation: Less accuracy than YOLO as Tiny YOLO model is smaller (Anish Aralikatti et al., 2020)",
  "has_answer": false
}