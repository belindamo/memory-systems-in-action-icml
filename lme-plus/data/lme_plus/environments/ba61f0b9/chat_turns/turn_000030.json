{
  "turn_global_index": 30,
  "session_index": 3,
  "session_id": "sharegpt_dTMQcnb_71",
  "turn_in_session": 2,
  "date": "2022/12/31 (Sat) 12:43",
  "role": "assistant",
  "content": "Sure, here's an example implementation of the `Trainer` class that loads the data and model, trains the model on the data, and saves the trained model:\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW, BertTokenizer\nfrom dataloader import get_loader\n\nclass Trainer:\n    def __init__(self, train_csv_path, val_csv_path, model_save_path, num_classes=8, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), num_workers=4, batch_size=64, max_len=512, learning_rate=2e-5, num_epochs=5):\n        \n        self.device = device\n        \n        # Load data\n        self.train_loader, self.val_loader = get_loader(train_csv_path, val_csv_path, tokenizer=BertTokenizer.from_pretrained('bert-base-cased'), max_len=max_len, batch_size=batch_size, num_workers=num_workers)\n        \n        # Load model\n        self.model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=num_classes).to(device)\n        \n        # Set up optimizer and loss function\n        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n        self.loss_fn = nn.CrossEntropyLoss()\n        \n        self.num_epochs = num_epochs\n        self.model_save_path = model_save_path\n        \n    def train(self):\n        for epoch in range(self.num_epochs):\n            train_loss = 0\n            train_acc = 0\n            self.model.train()\n\n            for i, batch in enumerate(self.train_loader):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                self.optimizer.zero_grad()\n\n                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                logits = outputs.logits\n\n                loss.backward()\n                self.optimizer.step()\n\n                # Calculate accuracy and accumulate loss\n                _, predictions = torch.max(logits, dim=1)\n                train_acc += torch.sum(predictions == labels)\n                train_loss += loss.item()\n\n            train_loss /= len(self.train_loader.dataset)\n            train_acc /= len(self.train_loader.dataset)\n\n            # Evaluate on validation set\n            val_loss = 0\n            val_acc = 0\n            self.model.eval()\n\n            with torch.no_grad():\n                for batch in self.val_loader:\n                    input_ids = batch['input_ids'].to(self.device)\n                    attention_mask = batch['attention_mask'].to(self.device)\n                    labels = batch['labels'].to(self.device)\n\n                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n                    logits = outputs.logits\n\n                    # Calculate accuracy and accumulate loss\n                    _, predictions = torch.max(logits, dim=1)\n                    val_acc += torch.sum(predictions == labels)\n                    val_loss += loss.item()\n\n                val_loss /= len(self.val_loader.dataset)\n                val_acc /= len(self.val_loader.dataset)\n\n            # Print loss and accuracy for train and validation sets\n            print(f'Epoch {epoch+1}/{self.num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n        # Save model\n        torch.save(self.model.state_dict(), self.model_save_path)\n```\nTo use this class to train the model, you can create an instance of the `Trainer` class and call its `train()` method:\n```python\ntrainer = Trainer(train_csv_path='./data\n```",
  "has_answer": false
}