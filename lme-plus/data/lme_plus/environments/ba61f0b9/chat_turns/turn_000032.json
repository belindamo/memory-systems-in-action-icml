{
  "turn_global_index": 32,
  "session_index": 3,
  "session_id": "sharegpt_dTMQcnb_71",
  "turn_in_session": 4,
  "date": "2022/12/31 (Sat) 12:43",
  "role": "assistant",
  "content": "I apologize for the mistake. You are correct that the function name is `get_data_loaders` not `get_loader`. Here's the corrected code:\n```python\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer\nfrom model import BertClassifier\n\nclass Trainer:\n    def __init__(self, train_csv_path, val_csv_path, num_classes, max_len=512, batch_size=64, num_workers=4, device='cuda'):\n        self.train_csv_path = train_csv_path\n        self.val_csv_path = val_csv_path\n        self.num_classes = num_classes\n        self.max_len = max_len\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n\n        # Load tokenizer\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n        # Set up data loaders\n        self.train_loader, self.val_loader = self.get_data_loaders()\n\n        # Set up model\n        self.model = BertClassifier(num_classes=num_classes).to(self.device)\n\n    def get_data_loaders(self):\n        train_dataset = NewsDataset(self.train_csv_path, self.tokenizer, self.max_len)\n        val_dataset = NewsDataset(self.val_csv_path, self.tokenizer, self.max_len)\n        train_sampler = DistributedSampler(train_dataset)\n        val_sampler = DistributedSampler(val_dataset)\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=self.num_workers, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, sampler=val_sampler, num_workers=self.num_workers, pin_memory=True)\n        return train_loader, val_loader\n\n    def train(self, num_epochs, lr=2e-5, weight_decay=1e-5):\n        # Set up optimizer and scheduler\n        optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n        num_training_steps = num_epochs * len(self.train_loader)\n        lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n        # Set up loss function and metrics\n        loss_fn = nn.CrossEntropyLoss()\n        train_loss = []\n        train_acc = []\n        val_loss = []\n        val_acc = []\n\n        # Train loop\n        for epoch in range(num_epochs):\n            self.model.train()\n            epoch_loss = 0.0\n            epoch_correct = 0.0\n            epoch_total = 0.0\n            for i, batch in enumerate(self.train_loader):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n\n                optimizer.zero_grad()\n                outputs = self.model(input_ids, attention_mask)\n                _, predicted = torch.max(outputs, dim=1)\n                loss = loss_fn(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                lr_scheduler.step()\n\n                epoch_loss += loss.item()\n                epoch_correct += torch.sum(predicted == labels)\n                epoch_total += labels.shape[0]\n\n                if i % 100 == 0:\n                    print(f'Train Epoch: {epoch}, Batch: {i}, Loss: {loss.item():.4f}, Accuracy: {(epoch_correct/epoch_total):.4f}')\n\n            train_loss.append(epoch_loss / len(self.train_loader))\n            train_acc.append(epoch_correct / epoch_total)\n\n            # Validation loop\n            self.model.eval()\n            with torch.no_grad():\n                epoch_loss = 0.0\n                epoch_correct = 0.0\n                epoch_total = 0.0\n                for batch in self.val_loader:\n                    input_ids =\n```",
  "has_answer": false
}