{
  "session_id": "sharegpt_zhBMAT7_21",
  "session_index": 38,
  "date": "2023/05/30 (Tue) 17:12",
  "turns": [
    {
      "role": "assistant",
      "content": "Certainly! Here is an example Python code to train a Transformer model for English to Chinese translation using the `transformers` library with multi-head attention and positional encoding:\n```python\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility\nrandom.seed(42)\ntorch.manual_seed(42)\n\n# Define a custom dataset for English-to-Chinese translation\nclass TranslationDataset(Dataset):\n    def __init__(self, src_texts, trg_texts, tokenizer):\n        self.src_texts = src_texts\n        self.trg_texts = trg_texts\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.src_texts)\n\n    def __getitem__(self, idx):\n        src_text = self.src_texts[idx]\n        trg_text = self.trg_texts[idx]\n\n        # Encode the source and target texts using the tokenizer\n        src_inputs = self.tokenizer(\n            src_text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\",\n        )\n        trg_inputs = self.tokenizer(\n            trg_text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\",\n        )\n\n        # Return the encoded inputs\n        return {\n            \"input_ids\": src_inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": src_inputs[\"attention_mask\"].squeeze(),\n            \"decoder_input_ids\": trg_inputs[\"input_ids\"].squeeze()[:-1],\n            \"decoder_attention_mask\": trg_inputs[\"attention_mask\"].squeeze()[:-1],\n            \"decoder_target_ids\": trg_inputs[\"input_ids\"].squeeze()[1:],\n        }\n\n# Define the training function\ndef train(model, train_loader, val_loader, epochs, learning_rate):\n    # Define the optimizer and loss function\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n    \n    # Train the model for the specified number of epochs\n    for epoch in range(1, epochs+1):\n        # Train the model for one epoch\n        model.train()\n        epoch_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n        for batch in progress_bar:\n            # Move the inputs and targets to the GPU\n            src = batch[\"input_ids\"].to(device)\n            src_mask = batch[\"attention_mask\"].to(device)\n            trg_input = batch[\"decoder_input_ids\"].to(device)\n            trg_target = batch[\"decoder_target_ids\"].to(device)\n            trg_mask = batch[\"decoder_attention_mask\"].to(device)\n\n            # Generate predictions for the batch\n            outputs = model(\n                input_ids=src,\n                attention_mask=src_mask,\n                decoder_input_ids=trg_input,\n                decoder_attention_mask=trg_mask,\n            )\n\n            # Compute the loss and perform backpropagation\n            loss = criterion(\n                outputs.logits.view(-1, outputs.logits.size(-1)),\n                trg_target.view(-1),\n            )\n            epoch_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        # Evaluate the model on the validation set\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                # Move the inputs and targets to the GPU\n                src = batch[\"input_ids\"].to(device)\n                src_mask = batch[\"attention_mask\"].to(device)\n                trg_input = batch[\"decoder_input_ids\"].to(device)\n                trg_target = batch[\"decoder_target_ids\"].to(device)\n                trg_mask =\n```"
    },
    {
      "role": "user",
      "content": "please continue the above code from \"trg\\_mask =\""
    },
    {
      "role": "assistant",
      "content": "batch[\"decoder\\_attention\\_mask\"].to(device)\n```python\n            # Generate predictions for the batch\n            outputs = model(\n                input_ids=src,\n                attention_mask=src_mask,\n                decoder_input_ids=trg_input,\n                decoder_attention_mask=trg_mask,\n            )\n\n            # Compute the loss\n            loss = criterion(\n                outputs.logits.view(-1, outputs.logits.size(-1)),\n                trg_target.view(-1),\n            )\n            val_loss += loss.item()\n\n    # Compute the average loss for the epoch\n    epoch_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n\n    # Print the training progress\n    print(f\"Epoch {epoch} train loss: {epoch_loss:.4f} val loss: {val_loss:.4f}\")\n```\nLoad the English-to-Chinese tokenizer and model\n===============================================\n\ntokenizer = AutoTokenizer.from\\_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\nmodel = AutoModelForSeq2SeqLM.from\\_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n\nPrepare the data\n================\n\nsrc\\_texts = [...] # list of English sentences\ntrg\\_texts = [...] # list of Chinese sentences\ndataset = TranslationDataset(src\\_texts, trg\\_texts, tokenizer)\ntrain\\_size = int(0.8 \\* len(dataset))\nval\\_size = len(dataset) - train\\_size\ntrain\\_dataset, val\\_dataset = torch.utils.data.random\\_split(dataset, [train\\_size, val\\_size])\ntrain\\_loader = DataLoader(train\\_dataset, batch\\_size=16, shuffle=True, num\\_workers=4)\nval\\_loader = DataLoader(val\\_dataset, batch\\_size=16, shuffle=False, num\\_workers=4)\n\nTrain the model\n===============\n\ndevice = torch.device(\"cuda\" if torch.cuda.is\\_available() else \"cpu\")\nmodel.to(device)\ntrain(model, train\\_loader, val\\_loader, epochs=10, learning\\_rate=2e-5)\n```javascript\n\nNote that the code assumes that you have already installed the `transformers` library and downloaded the pre-trained `Helsinki-NLP/opus-mt-en-zh` English-to-Chinese model. If you have not done so, you can install the library with `pip install transformers` and download the model with `AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")`. Also note that you will need to replace the placeholder `src_texts` and `trg_texts` lists with your own English and Chinese sentence pairs.\n```"
    }
  ]
}