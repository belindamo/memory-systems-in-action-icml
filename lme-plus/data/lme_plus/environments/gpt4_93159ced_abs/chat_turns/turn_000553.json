{
  "turn_global_index": 553,
  "session_index": 38,
  "session_id": "sharegpt_zhBMAT7_21",
  "turn_in_session": 2,
  "date": "2023/05/30 (Tue) 17:12",
  "role": "assistant",
  "content": "batch[\"decoder\\_attention\\_mask\"].to(device)\n```python\n            # Generate predictions for the batch\n            outputs = model(\n                input_ids=src,\n                attention_mask=src_mask,\n                decoder_input_ids=trg_input,\n                decoder_attention_mask=trg_mask,\n            )\n\n            # Compute the loss\n            loss = criterion(\n                outputs.logits.view(-1, outputs.logits.size(-1)),\n                trg_target.view(-1),\n            )\n            val_loss += loss.item()\n\n    # Compute the average loss for the epoch\n    epoch_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n\n    # Print the training progress\n    print(f\"Epoch {epoch} train loss: {epoch_loss:.4f} val loss: {val_loss:.4f}\")\n```\nLoad the English-to-Chinese tokenizer and model\n===============================================\n\ntokenizer = AutoTokenizer.from\\_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\nmodel = AutoModelForSeq2SeqLM.from\\_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n\nPrepare the data\n================\n\nsrc\\_texts = [...] # list of English sentences\ntrg\\_texts = [...] # list of Chinese sentences\ndataset = TranslationDataset(src\\_texts, trg\\_texts, tokenizer)\ntrain\\_size = int(0.8 \\* len(dataset))\nval\\_size = len(dataset) - train\\_size\ntrain\\_dataset, val\\_dataset = torch.utils.data.random\\_split(dataset, [train\\_size, val\\_size])\ntrain\\_loader = DataLoader(train\\_dataset, batch\\_size=16, shuffle=True, num\\_workers=4)\nval\\_loader = DataLoader(val\\_dataset, batch\\_size=16, shuffle=False, num\\_workers=4)\n\nTrain the model\n===============\n\ndevice = torch.device(\"cuda\" if torch.cuda.is\\_available() else \"cpu\")\nmodel.to(device)\ntrain(model, train\\_loader, val\\_loader, epochs=10, learning\\_rate=2e-5)\n```javascript\n\nNote that the code assumes that you have already installed the `transformers` library and downloaded the pre-trained `Helsinki-NLP/opus-mt-en-zh` English-to-Chinese model. If you have not done so, you can install the library with `pip install transformers` and download the model with `AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")`. Also note that you will need to replace the placeholder `src_texts` and `trg_texts` lists with your own English and Chinese sentence pairs.\n```",
  "has_answer": false
}