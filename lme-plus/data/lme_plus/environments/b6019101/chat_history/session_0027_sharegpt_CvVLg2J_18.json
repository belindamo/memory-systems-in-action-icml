{
  "session_id": "sharegpt_CvVLg2J_18",
  "session_index": 27,
  "date": "2023/05/09 (Tue) 18:53",
  "turns": [
    {
      "role": "user",
      "content": "Figure 4 provides an overview of our fndings on SAV deployment,\nfor packets both outbound from and inbound to the measured network, for the year ending 1 August 2019. We present deployment\nstatistics by IPv4/24 and IPv6/40 prefxes, and by AS. An AS with\npartial deployment originates some prefxes from which the Spoofer\nsystem did not receive spoofed packets, and originates other prefxes from which the Spoofer system did.\nThis data indicates that while most networks block packets with\nspoofed source IP addresses, deployment is far from universal. In\nparticular, 25.2% and 32.1% of tested IPv4 and IPv6 ASes, respectively, had at least one prefx where operators had not deployed SAV\nto block outbound spoofed packets to the Internet (frst and last\nbars in fgure 4b). The comparatively small fraction of prefxes from\nwhich we received spoofed IPv4 packets (fgure 4a) is primarily due\nto the presence of Network Address Translation (NAT) routers that\nrewrite spoofed packets with the router\u2019s publicly routable address.\nWhen a NAT router was not present, 14.9% of IPv4 prefxes had no\nfltering; fltering was better deployed at prefx-level granularity in\nIPv6, with 12.3% of tested prefxes not fltering. Outbound from Client (AS) Inbound to Client (prefix) Inbound to Client (AS)\nOutbound from Client (prefix)\n 80 100\nIPv4\nIPv4 NAT\nIPv4 No\u2212NAT\nIPv6\nIPv4\nIPv4 NAT\nIPv4 No\u2212Nat\nIPv6\nIPv4\nIPv6\nIPv4\nIPv6\n 0 20 40 60 80 100\n7.3%\n6.4%\n14.9%\n12.3%\n11.9\u221225.2%\n22.0%\n21.1\u221230.5%\n24.5\u221232.1%\n42.6%\n62.2%\n59.2\u221267.0%\n68.6\u221274.2%\n(a)\n(c)\n(d)\nPercentage with Classification\n 0\nPartial Rewritten Blocked\n(b)\nNo Filtering\n 20 40 60\nFigure 4: Summary statistics for Spoofer Project for the year\nending August 2019. Percent values to right of barplot indicate the fraction of networks with problematic SAV deployments, either no fltering or partial fltering of spoofed packets (red plus orange segments).\nThe lower panels of fgure 4 (c and d) summarize the observed\nstate of fltering of packets inbound to the client\u2019s network, claiming\nto be from within the same subnet as the client. This test sends a\npacket to the client with a source address that is inside the same\nsubnet as the client. For IPv4 we toggle the last (31st) bit of the\naddress, and for IPv6 we toggle the 120th bit. Surprisingly, inbound\nfltering of spoofed packets is even less deployed than outbound fltering, despite these packets being a threat to the receiving network.\nDeploying this type of fltering is incentive-compatible because internal hosts are often more trusted than external hosts [46]. In our\ndata, 67.0% and 74.2% of IPv4 and IPv6 ASes, respectively, had at\nleast one prefx where they were not fltering inbound packets.\nWe compare the IPv4 tests we received for the year ending August 2019 where the client was not behind a NAT, to the same class\nof tests 1 May 2006 to 7 May 2006, during which the project received\n1057 tests, triggered by an article on Slashdot encouraging readers\nto run the original Spoofer test [61] (lower left panel of fgure 2).\nFor the frst week of May 2006, 18.3% of IPv4/24 prefxes and 20.4%\nof ASes tested did not block spoofed-source packets from leaving\ntheir networks. Figure 4 shows that for the year ending August\n2019, 14.9% of IPv4/24 prefxes and 30.5% of ASes tested did not\nblock spoofed-source packets, implying that SAV deployment has\nnot generally improved.\nWe reinforce this result by considering the infuence of a prefx\u2019s\nage \u2013 defned as how long it has been observable in the global\nBGP routing table \u2013 on the probability that it performs source address validation. One hypothesis is that older networks are more\n 0.05\n 0.1\n 0.15\n 0.2\n 0 50 100 150 200 250\nPrefix Age (Months)\nFraction of Spoofable Tests\n 0\nf(x) = 0.0000262x + 0.08\nFigure 5: For Spoofer tests collected between May 2016 and\nAugust 2019, we found no correlation between prefx age\n(measured as frst appearance in the global BGP routing table) and its probability of SAV deployment (as measured by\na client in that prefx performing a test).\nmature and hence have better hygiene. However, newer networks\nmay have newer equipment, less confguration inertia, and better\nawareness of network security. The increasing prevalence of address transfers post-IPv4-exhaustion adds additional noise to this\nalready speculative exercise.\nWe generate a radix trie of prefx ages by iterating over monthly\nsnapshots of Routeviews [1] and RIPE RIS from January 2000 to\nAugust 2019. The meta-data in the radix trie for each prefx is the\ndate it frst appeared in our snapshots. Note that prefx aggregation\nand deaggregation over time result in new ages within the radix\ntrie \u2013 i.e., a deaggregation is a new BGP policy and hence resets the\nage. Let t0(p) be the time at which the IPv4 prefx p frst appeared\nin the global BGP routing table. For a client with IPv4 address c\nwho performs a Spoofer test at time t, we fnd the longest matching\nprefx p\n\u2032 = LPM(c,t) within the radix trie at time t. The prefx age\nis then relative to the time of the test:\na\u0434e(c,t) = t \u2212 t0(LPM(c,t))\nFor each Spoofer client using a routed source address, we determined the age of the corresponding client prefx. We binned results\ninto months and determined the relative fraction of tests within\neach age bin that could successfully spoof the routed IPv4 address.\nFigure 5 shows the relationship between the fraction of tests where\na client could spoof and the age of the prefx to which the client\nbelonged. Using Pearson\u2019s correlation, we found no meaningful\nrelationship between age and spoofability.\n5 EPISTEMOLOGICAL CHALLENGES WITH\nCROWD-SOURCING MEASUREMENT\nA crowd-sourced approach to assessing the deployment of SAV\nis confounded by multiple factors. First, the opt-in nature of the\nmeasurement can induce sample bias: people interested in security issues are more likely to provide a measurement. Second, and\nrelated, the measurement results we receive are distributed nonuniformly across time and networks. Third, the Internet itself is\nchanging, e.g., due to equipment upgrades, confguration changes,\nand policy changes. Fourth, the results of a single test may not\nbe indicative of the larger network, prefx, or autonomous system\nfrom which the client executes its test.\n 140\nWeeks Active\nCumulative Fraction\nLongest matching prefix\n 0\n 0.2\n 0.4 AS\n 0.6\n 0.8\n 1\n 0 20 40 60 80 100 120\n/24 prefix\nFigure 6: Even with periodic weekly probing by the client,\nonly 8% of IPv4 /24 prefxes and 23% of ASes reported measurements in 20 or more weeks in data we collected between\nMay 2016 and August 2019.\nThis sparsity complicates inferences of remediation. For instance,\nif two samples from an AS are signifcantly separated in time and\nfrom diferent prefxes, and the frst sample indicates the tested\nnetwork permits spoofng, while the second indicates fltering, there\ncould be several diferent explanations for the change. The tested\nnetwork might have deployed SAV across their network, fxing the\nfrst prefx; or the frst tested prefx might still permit spoofng.\n5.1 Efect of daemonizing Spoofer client\nWe attempted to mitigate sampling concerns by daemonizing the\nSpoofer client, i.e., running the client in the background after installation, executing measurements any time the client detected a\nnew attached network, and repeating tests weekly on previously\nseen networks. In addition to obtaining more test samples, daemonizing the Spoofer client allows the system to automatically gather\nlongitudinal samples of the same network \u2013 data that is useful to\ncharacterize the evolution of fltering policies.\nBecause we do not use a client identifer (\u00a73) it is not possible\nto determine whether a specifc client continues to probe after its\ninitial installation. Instead, we must estimate the ability of the periodic daemon to gather longitudinal data for a given /24 prefx,\nlongest matching BGP prefx, or AS. Figure 6 displays the cumulative fraction of these as a function of the number of active weeks.\nWe defne an active week to be any week for which there is a test\nreport from a client with the daemon functionality.\nUnfortunately, even with the daemon, 44% of the IPv4 /24 prefxes\nand 32% of the ASes reported measurements in only a single week\nbetween May 2016 and August 2019. This may be due to either\nusers who run the tool once and then uninstall it after obtaining\ntheir test result, or infrequent or one-of tests performed by clients\nwhile they are roaming, e.g., mobile hosts that attach infrequently\nto a hotel or airport network. Although unexpected, these results\nare also consistent with feedback from operators that told us they\nused the client on a short-term basis while they were testing SAV\nconfguration and then uninstalled the software from their laptop.\nWhile our coverage is longitudinally sparse for many networks,\nfgure 6 exhibits long tails. In particular, for large providers, we\nhave tests in every week. For example, we obtained test results from\nclients in AT&T, Comcast, Verizon, Deutsche Telekom, Charter, and\nCox in nearly all of the weeks since the release of the daemonized\nclient \u2013 May 2016 to August 2019.5.2 Representativeness of Spoofer data\nAnother inferential challenge of crowd-sourced measurement is\nassessing how representative the data is of the networks it samples,\nand the larger Internet [21]. Since the measurements we receive\ncome from volunteers, both on-demand and triggered by clients\ndetecting new network attachments, we do not control the sampling\nacross either networks or time. While we can characterize the\ncoverage of our results by, e.g., networks, ASes, or geolocation,\nthis characterization does not capture the degree to which our\nmeasurements correctly capture the behavior of those networks.\nTo gain confdence in the data\u2019s representativeness of the networks it covers, we assess the data\u2019s predictive ability \u2013 i.e., whether\nwe can use the data to predict the ability of a client to spoof using\nan IP address we have no prior tests from. We use the standard\ntrain-then-test supervised learning approach, but always split the\ntraining and test sets such that the training samples are chronologically before the test samples. We analyze the data on month\nboundaries, such that the model is trained on all data prior to a\ngiven month and tested on all data for the month and after. In this\nway, we simulate predictions over future tests given the available\ndata we have to that point.\nWe built a simple per-prefx model of spoofability that operates\nin ascending chronological order over training samples. Our algorithm determines the global BGP prefx from which the client ran\nthe test using an August 2019 RIB snapshot from route-views2 [1].\nWe then label (or update) the prefx in a radix-trie with the client\u2019s\nspoofng status. To account for variations across policies of individual networks within larger BGP prefxes, we also label the spoofng\nstatus of the more specifc /24 prefx of the client. The model also\ncomputes the network-wide prior probability of spoofng using the\nfraction of clients that can spoof for each source address.\nAfter building the radix-trie using the training data, the model\nqueries the trie in a manner analogous to a routing table lookup\nover the test data. For each new client IP address in the test set, the\nmodel returns a prediction corresponding to the spoofng capability\nof its longest-prefx match on the radix trie. In the case that there\nis no matching longest-prefx, the model fips a biased coin that\ncaptures the overall prior probability of being able to spoof. Note\nthat we ignore clients in the test set that also occur in the training\nset, so as not to artifcially infate the results positively via simple\nmemorization of previous results from that same client IP address.\nThus, we only test over clients that are \u201cnew.\u201d\nWe term the ability to send spoofed packets a positive label. Our\nmodel\u2019s accuracy is > 90% across IP versions and source addresses.\nHowever, accuracy is a poor measure of the model due to the fact\nthat the prior probability of spoofability is low, so we focus on\nrecall and precision. Figure 7 plots per-prefx spoofability recall and\nprecision of our model, as a function of the month that splits the\ntraining from the test data. Recall is a measure of positive coverage:\nthe fraction of the clients able to spoof for which the model correctly\npredicted this fact. Precision (positive predictive value) captures\nfdelity: the fraction of the clients the model predicted can spoof\nwhich were actually observed to spoof.\nThis data-driven model is basic and could incorporate many additional features, or use more advanced statistical learning techniques.\nThus, our recall and precision results represent a lower-bound of\nwhat is likely possible to model. Rather, our goal is to demonstrate:\ni) that our automated data gathering eforts have increased the\nquality of the model over time; and ii) the data we have efectively\ncaptures the actual state of Internet spoofability, by virtue of its\nability to make accurate predictions for unknown clients based on\nthe prior population of client data available to the project.\nFigure 7(a) shows that as the system has accumulated more tests,\nthe model\u2019s predictive recall has increased signifcantly, from a low\nof around 20% in May 2016, to more than 80% for IPv6 in August\n2019, but only 50-70% for IPv4. Similarly, fgure 7(b) shows that our\nmodel\u2019s precision has increased signifcantly in the last year, and\nIPv6 predictions outperform IPv4 predictions. Note that 76% and\n87% of the predictions come from the model\u2019s radix trie for August\n2019, for IPv4 and IPv6 respectively. Thus, the model\u2019s performance\ndoes not come from making random guesses based on the overall\nprior spoofng probability, but from topological characteristics in\nthe observed data.\nThe model has poor precision for IPv4 private addresses, because\nfltering for IPv4 private addresses is more pervasive in the core,\nwhich occludes measurement of fltering policies at the edge. Figure 8 shows that for all other classes of spoofed source address\npackets, nearly all Spoofer servers distributed around the world\nreceive the class of spoofed packet. This is typically never the case\nwith IPv4 private-address packets, which are fltered elsewhere in\nthe network, rather than at the source.\n6 NATS ARE NOT A SUBSTITUTE FOR SAV\nOur testing client is primarily installed on hosts whose IPv4 connectivity is provided by a NAT [18] system. In the year ending August\n2019, we received reports from 2,783 ASes using IPv4; 2,418 (86.8%)\nof these ASes had tests involving a NAT. A NAT system rewrites\nthe source address of packets so that internal hosts behind the NAT\nwith private addresses can communicate with other systems across\nthe Internet. A well-implemented NAT should not forward packets\nwith spoofed source addresses, as the spoofed address is unlikely\nto fall within the private address range the NAT serves. However,\nfgure 4a shows that 6.4% of IPv4 /24 prefxes tested from clients\nbehind a NAT did not flter packets with spoofed source addresses\nin the year ending August 2019. Further, NAT use is rare in IPv6 because unique addresses are plentiful, so SAV is explicitly required in\nIPv6 to flter packets with spoofed source addresses; fgure 4a also\nshows that 12.3% of tested IPv6 /40 prefxes did not flter packets\nwith spoofed source addresses over the same time period.\n6.1 IPv4 NATs are broken\nIn practice, there are two failure modes where a NAT will forward\npackets with spoofed source addresses. We illustrate these failure\nmodes in fgure 9. In the frst failure mode, the NAT simply forwards\nthe packets with the spoofed source address (the victim) intact to\nthe amplifer. We hypothesize this occurs when the NAT does not\nrewrite the source address because the address is not in the local\nnetwork served by the NAT. In the second failure mode, the NAT\nrewrites the source address to the NAT\u2019s publicly routable address,"
    },
    {
      "role": "assistant",
      "content": "The paper \"Network Hygiene, Incentives, and Regulation: Deployment of Source Address Validation in the Internet\" presents data on the deployment of source address validation (SAV) on the internet. The data is presented in Figure 4, which provides an overview of the findings on SAV deployment for packets both outbound from and inbound to the measured network, for the year ending August 1, 2019. The data is presented by IPv4/24 and IPv6/40 prefixes, and by autonomous systems (ASes). The figure shows that while most networks block packets with spoofed source IP addresses, deployment is far from universal. The data indicates that 25.2% and 32.1% of tested IPv4 and IPv6 ASes, respectively, had at least one prefix where operators had not deployed SAV to block outbound spoofed packets to the Internet. The authors also show that the deployment of SAV is worse for inbound packets than outbound packets, despite these packets being a threat to the receiving network. The authors also compare the data from the year ending August 2019 to data from May 2006, and found that the deployment of SAV has not generally improved."
    }
  ]
}