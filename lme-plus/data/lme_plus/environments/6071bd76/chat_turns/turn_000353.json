{
  "turn_global_index": 353,
  "session_index": 34,
  "session_id": "sharegpt_7mgd8B5_0",
  "turn_in_session": 1,
  "date": "2023/07/15 (Sat) 07:38",
  "role": "assistant",
  "content": "To predict the GR value for each material using the Temporal Fusion Transformer (TFT) with PyTorch Forecasting, you should follow these steps:\n\n1. Install PyTorch Forecasting:\n```\npip install pytorch-forecasting\n```\n1. Import necessary libraries:\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport torch\nfrom pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, BaseModel\nfrom pytorch_forecasting.data import GroupNormalizer\nfrom pytorch_forecasting.metrics import SMAPE\nfrom pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n```\n1. Load and preprocess the dataset:\n```python\n# Load the dataset (use your own data source)\ndata = pd.read_csv(\"data.csv\")\n\n# Convert date columns to datetime\ndata['SnapshotDate'] = pd.to_datetime(data['SnapshotDate'])\n\n# Map Product Category to integers\ndata['Product Category'] = data['Product Category'].astype('category').cat.codes\n\n# Create a time index for the dataset\ndata['time_idx'] = (data['SnapshotDate'].dt.year - 2022) * 12 + data['SnapshotDate'].dt.month\n\n# Set the max_prediction_length and max_encoder_length\nmax_prediction_length = 6\nmax_encoder_length = 12\n\n# Create a training dataset\ntraining_cutoff = data['time_idx'].max() - max_prediction_length\n\n# Split the data into training and validation\ntraining = TimeSeriesDataSet(\n    data[data.time_idx <= training_cutoff],\n    time_idx=\"time_idx\",\n    target=\"Cumulative GR\",\n    group_ids=[\"Material\"],\n    min_encoder_length=max_encoder_length,\n    max_encoder_length=max_encoder_length,\n    min_prediction_length=max_prediction_length,\n    max_prediction_length=max_prediction_length,\n    static_categoricals=[\"Product Category\"],\n    static_reals=[],\n    time_varying_known_categoricals=[],\n    time_varying_known_reals=[\"time_idx\", \"SO_CSD\", \"SO_CRD\", \"PO_CSD\", \"PO_CRD\", \"Billing\"],\n    time_varying_unknown_categoricals=[],\n    time_varying_unknown_reals=[\"Cumulative GR\"],\n    target_normalizer=GroupNormalizer(groups=[\"Material\"], transformation=\"softplus\"),\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n)\n\n# Create a validation dataset\nvalidation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n```\n1. Find optimal hyperparameters:\n```python\n# Find the optimal hyperparameters\nstudy = optimize_hyperparameters(\n    training,\n    validation,\n    model_path=\"optuna_test\",\n    n_trials=100,\n    max_epochs=50,\n    gradient_clip_val_range=(0.01, 1.0),\n    hidden_size_range=(8, 128),\n    hidden_continuous_size_range=(8, 128),\n    attention_head_size_range=(1, 4),\n    learning_rate_range=(0.001, 0.1),\n    dropout_range=(0.1, 0.3),\n    trainer_kwargs=dict(limit_train_batches=30),\n    reduce_on_plateau_patience=4,\n    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate\n)\n```\n1. Train the model:\n```python\n# Instantiate the Temporal Fusion Transformer with optimal hyperparameters\nmodel = TemporalFusionTransformer.from_hyperparameters(study.best_trial.params)\n\n# Train the model\ntrainer = torch.optim.swa_utils.SWALR(\n    model,\n    anneal_strategy=\"cos\",\n    anneal_epochs=5,\n    swa_lr=study.best\n```",
  "has_answer": false
}