{
  "turn_global_index": 198,
  "session_index": 18,
  "session_id": "sharegpt_XaM55h6_28",
  "turn_in_session": 0,
  "date": "2023/06/19 (Mon) 18:39",
  "role": "user",
  "content": "summarize the below texts to bullet point sentences\n\n3. Method\nOur approach builds on data2vec (Baevski et al., 2022) and\nwe first describe the major shared techniques including pre\u0002dicting contextualized target representations (\u00a73.1). Similar\nto Masked Autoencoders (MAE; He et al. 2021), we encode\nonly non-masked portions of a sample and use a decoder\nmodel to predict target representations for the masked por\u0002tions but instead of using a Transformer-based decoder, we\nuse a smaller convolutional decoder which we find to be eas\u0002ier and faster to train (\u00a73.2). To amortize the computational\noverhead of creating contextualized target representations,\nwe reuse each target for multiple masked versions of a train\u0002ing sample (\u00a73.3) and instead of random masking or block\nmasking, our inverse block masking strategy ensures that\ncontiguous regions of the sample are preserved to provide\nmore context for student predictions (\u00a73.4).\n3.1. Contextualized Target Prediction\nInstead of reconstructing local windows of the the raw input\ndata (He et al., 2021), or predicting discrete representations\nthereof (Bao et al., 2021), we predict representations of the\nteacher network incorporating information from the entire\ninput sample. This leads to a richer training task where\ntargets are specific to a particular training sample. Contex\u0002tualized targets are built via the self-attention mechanism\nof a Transformer-based teacher model which encodes the\nunmasked training sample (Paulus et al., 2017; Vaswani\net al., 2017) and the training targets are a weighted sum of\nall features in the sample.\nTarget Representations and Learning Objective. Train\u0002ing targets are based on averaging the top K FFN blocks\nof the teacher. Before averaging, activations are normal\u0002ized using instance normalization (Ulyanov et al., 2016).1\nThe training task is for the student network to regress these\ntargets based on the masked version of the sample.\nTeacher Weights. The teacher weights \u2206 are an expo\u0002nentially moving average of the student encoder weights \u03b8\n(Grill et al., 2020): \u2206 \u2190 \u03c4\u2206 + (1 \u2212 \u03c4 ) \u03b8 where \u03c4 follows\n1Layer normalization (Ba et al., 2016) of the averaged targets\ncan be useful for some modalities such as speech and vision.\na linearly increasing schedule from a starting value \u03c40 to a\nfinal value \u03c4e over \u03c4n updates, after which the value is kept\nconstant (Baevski et al., 2022).\nLearning Objective. We use an L2 loss based on the\ntarget representation from the teacher network y and the\nstudent network prediction f(x). This is a simplification\ncompared to the Smooth L1 loss used in Baevski et al. (2022)\nand we found it to work well across modalities.\n3.2. Model Architecture\nSimilar to data2vec (Baevski et al., 2022), our model uses\nmodality-specific feature encoders and a Transformer archi\u0002tecture where the latter makes up the the bulk of the model\nweights (Vaswani et al., 2017). For computer vision, we use\na patch mapping of 16x16 pixels as feature encoder (Doso\u0002vitskiy et al., 2020), for speech a multi-layer convolutional\nnetwork following van den Oord et al. (2018); Baevski et al.\n(2020b; 2022) and for text we use embeddings learned based\non byte-pair encoding (Sennrich et al., 2016).\nAsymmetric Encoder/Decoder Architecture. In a first\nstep, we use the teacher network to encode all parts of the\nunmasked training sample in order to create training targets\n(\u00a73.1). Next, we mask part of the sample (\u00a73.4) and em\u0002bed it with the student encoder. To improve efficiency, we\nencode only unmasked patches or time-steps of a training\nexample which leads to a large speed-up compared to encod\u0002ing all parts of the sample (He et al., 2021), depending on\nthe amount of masking. The output of the student encoder is\nthen merged with fixed representations for the masked por\u0002tions and fed to a decoder network. To represent the masked\ntokens, we found it sufficient to use random Gaussian noise\ncompared to a learned representation (He et al., 2021).2 The\ndecoder network then reconstructs the contextualized target\nrepresentation of the teacher network for time-steps which\nare masked in the student input.\nConvolutional Decoder Network. We use a lightweight\ndecoder consisting of D convolutions, each followed by\nlayer normalization (Ba et al., 2016), a GELU activation\nfunction (Hendrycks & Gimpel, 2016), and a residual con\u0002nection (He et al., 2015). For sequential data such as speech\nand text we use 1-D convolutions and for images we use\n2-D convolutions, each parameterized by groups to increase\nefficiency (Krizhevsky et al., 2012). We tune the number of\nlayers and kernel size for each modality.\n2We also experimented with adding positional embeddings but\nfound that they do not improve results.\nEfficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language\n3.3. Multi-mask Training\nA disadvantage of the data2vec teacher-student setup is the\nneed to process each sample twice: once to obtain targets\nwith the teacher model, and once to obtain predictions of\nthe student. Moreover, computing activations for the teacher\nmodel is also less efficient compared to the student model\nsince the teacher needs to process the full unmasked input.3\nIn order to amortize the cost of the teacher model com\u0002putation, we reuse the teacher representation for multiple\nmasked versions of the training sample. Concretely, we con\u0002sider M different masked versions of the training sample\nand compute the loss with respect to the same target repre\u0002sentation. This is possible, because target representations\nare based on the full unmasked version of the sample. As\nM grows, the computational overhead of computing target\nrepresentations becomes negligible. In practice, this enables\ntraining with a relatively small batch size compared to other\nself-supervised work (\u00a74).\nConsidering multiple masked versions of a training sam\u0002ple has been previously explored in the context of self\u0002supervised learning for computer vision with ResNet mod\u0002els (Jing et al., 2022), although the authors found that it\nperformed much less well than different image augmenta\u0002tions. Caron et al. (2020a) considers multiple crops based\non the same image but trains the model by comparing dis\u0002crete codes rather than predicting the representation of the\noriginal image. And Girdhar et al. (2022) trains MAE mod\u0002els on videos with multiple masked versions of a sample to\namortize the overhead of data loading and preparation.\nAnother efficiency improvement of data2vec 2.0 compared\nto data2vec is to share the feature encoder output across the\ndifferent masked versions of the training example to avoid\nredundant computation. This leads to significant speed\u0002ups for dense modalities such as speech where the feature\nencoder accounts for a large portion of the computation but\nless so for other modalities such as text.\n3.4. Inverse Block Masking\nThe MAE-style sample encoding improves efficiency but\nalso removes the ability to store information in the acti\u0002vations of masked time-steps which makes the training\ntask more challenging. Random masking is successful for\nMasked Autoencoders (He et al., 2021) but it may interfere\nwith the ability to build semantic representations since there\nis no structure in the masks that are created. Block mask\u0002ing (Bao et al., 2021) is more structured by masking entire\nblocks of time-steps or patches but there is no guarantee\nthat large contiguous portions of the training sample are\nunmasked. Our goal is to enable the student model to build\n3Baevski et al. (2022) found it important to build targets based\non the unmasked sample rather than another masked version.\nsemantically rich representations over local regions of the\nsample.\nWe therefore introduce inverse block masking: instead of\nchoosing which patches to mask, it chooses which patches\nto preserve in a block-wise fashion, where the size of a block\nis in terms of the number of patches or time-steps B. We\nfirst sample the starting point of each block to keep, and\nthen expand it symmetrically until the block is of width B,\nfor speech and text, or \u221a\nB for images.4 We sample the\nfollowing number of starting points without replacement\nand expand them to width\n\u221a\nB or quadratic blocks of width\nB, depending on the modality:\nL \u00d7\n(1 \u2212 R) + A\nB\nwhere L is the total number of time-steps/patches in a train\u0002ing sample, R is the mask ratio, a hyper parameter control\u0002ling the percentage of the sample that is masked and A is a\nhyper-parameter to adjust mask ratio (see below).\nWe allow blocks to overlap, which results in over-masking\nand some variance in the number of actually masked time\u0002steps for each sample. Since we only encode unmasked\ntime-steps, we use a simple strategy to assimilate the number\nof unmasked time-steps for all samples in a batch: for each\nsample, we randomly choose individual time-steps to mask\nor unmask until we reached the desired number of unmasked\ntime-steps L \u00d7 (1 \u2212 R).",
  "has_answer": false
}