{
  "turn_global_index": 400,
  "session_index": 43,
  "session_id": "sharegpt_VWOHFxG_0",
  "turn_in_session": 2,
  "date": "2023/05/29 (Mon) 10:49",
  "role": "user",
  "content": "Improve the following: 1. Motivations and Goals\n\nSemantic Communication (SC) for 6G: Semantic communication (SC) is an emerging field that has great potential in revolutionizing our current approaches to designing communication architectures and algorithms towards enabling truly intelligent and sustainable 6G systems [JSAC]. While the current communication system rooted in Shannon\u2019s theory [Math], hereafter referred to as Shannon communication, focuses on accurately reproducing source data in bits at a remote location, SC additionally pre-processes the bits for extracting and delivering the meanings of the bits, i.e., semantics, and post-processes the received semantics to maximize their effectiveness in a given application-specific task (e.g., reconstruction, classification, control, and other decision-making). Semantics can be the raw data\u2019s generative process that is invariant to source data realizations (e.g., a \u2018cat\u2019 image and a word \u201ccat\u201d). Such semantics are not only smaller than their generated source data, but also can generate different types of source data. Leveraging this invariant property of semantics enables data-type transformation (e.g., text-to-image) that greatly reduces communication payload sizes and latency.\n\nSystem 1 SC \u2013 Minimal, Invariant, and Sufficient SR Communication: Recently, several SC methods have been proposed [Walid, Kaibin, Geoffrey], among which deep learning based SC (DeepSC), hereafter referred to as System 1 SC, is arguably the most widely accepted framework. System 1 SC implements the aforementioned semantic pre/post-processing via the autoencoder (AE) architecture. AE is a neural network (NN) comprising a pair of an encoder and a decoder, and aims to reconstruct the encoder\u2019s raw data input at the decoder\u2019s output by exchanging hidden-layer outputs. These hidden-layer outputs can be interpreted as the sampled representations of the raw data\u2019s generative process, hereafter referred to as semantic representations (SRs). Under the AE architecture, the success of System 1 SC rests on learning SRs that are minimal, invariant, and sufficient to carry out a given task. System 1 SC has shown that it is possible to construct such desired SRs from complex real data in vision, audio, and other high-stake applications [REF]. However, these existing works have studied commonly on data reconstruction tasks in a point-to-point scenario [Geoffrey], and the feasibility and scalability of System 1 SC in other tasks are questionable. As Table 1 summarizes, the fundamental limitations of System 1 SC emanate from its data-driven nature, in which the AE is inherently biased towards the data, channels, and tasks considered during training. Furthermore, AE is intrinsically structured for reconstruction tasks in the way that the decoder is a mirror image of the encoder, which is ill-suited for other tasks. Lastly, an NN is a trainable function that can be neither instantly revised nor flexibly intervened, without an iterative process of re-training.",
  "has_answer": false
}